{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-05T09:01:45.818923Z","iopub.execute_input":"2023-11-05T09:01:45.819850Z","iopub.status.idle":"2023-11-05T09:01:45.835254Z","shell.execute_reply.started":"2023-11-05T09:01:45.819808Z","shell.execute_reply":"2023-11-05T09:01:45.834355Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/english-sentences-30000/english_sentences_AIproject.xlsx\n","output_type":"stream"}]},{"cell_type":"code","source":"# import necessary libraries and moduels\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:01:46.923278Z","iopub.execute_input":"2023-11-05T09:01:46.924186Z","iopub.status.idle":"2023-11-05T09:01:50.489926Z","shell.execute_reply.started":"2023-11-05T09:01:46.924148Z","shell.execute_reply":"2023-11-05T09:01:50.488550Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# import the dataset\ndf=pd.read_excel(\"/kaggle/input/english-sentences-30000/english_sentences_AIproject.xlsx\")","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:01:51.735119Z","iopub.execute_input":"2023-11-05T09:01:51.736179Z","iopub.status.idle":"2023-11-05T09:01:54.219753Z","shell.execute_reply.started":"2023-11-05T09:01:51.736139Z","shell.execute_reply":"2023-11-05T09:01:54.218920Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(df.head(5))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:01:56.158425Z","iopub.execute_input":"2023-11-05T09:01:56.159662Z","iopub.status.idle":"2023-11-05T09:01:56.171164Z","shell.execute_reply.started":"2023-11-05T09:01:56.159625Z","shell.execute_reply":"2023-11-05T09:01:56.170246Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"   Sr No                                          Sentences\n0      1                             I have to go to sleep.\n1      2  Today is June 18th and it is my sister's birth...\n2      3                        I am going to turn 20 soon.\n3      4              You should never share your password.\n4      5                               I will be back soon.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert into list of sentences\nsentences = df['Sentences'].dropna().tolist()","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:02:01.140244Z","iopub.execute_input":"2023-11-05T09:02:01.141019Z","iopub.status.idle":"2023-11-05T09:02:01.150258Z","shell.execute_reply.started":"2023-11-05T09:02:01.140984Z","shell.execute_reply":"2023-11-05T09:02:01.149284Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Create an instance of Tokenizer class\ntokenizer = Tokenizer()\n\n#Fit the tokenizer on the list of Sentences\ntokenizer.fit_on_texts(sentences)\n\n#Calculate the total number of words in the dataset\ntotal_words = len(tokenizer.word_index) + 1\ninput_sequences = []","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:02:02.348938Z","iopub.execute_input":"2023-11-05T09:02:02.349328Z","iopub.status.idle":"2023-11-05T09:02:02.822946Z","shell.execute_reply.started":"2023-11-05T09:02:02.349294Z","shell.execute_reply":"2023-11-05T09:02:02.821875Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"for line in sentences:\n    # Skip empty lines\n    if pd.notna(line):\n        \n        # Tokenize the current line into list of integer sequences\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            \n            # Create n-gram sequence \n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:02:05.858875Z","iopub.execute_input":"2023-11-05T09:02:05.859269Z","iopub.status.idle":"2023-11-05T09:02:06.833381Z","shell.execute_reply.started":"2023-11-05T09:02:05.859236Z","shell.execute_reply":"2023-11-05T09:02:06.832279Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Pad the sequence to ensure consistent length of each sequence\nmax_sequence_len = max([len(seq) for seq in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:02:10.183093Z","iopub.execute_input":"2023-11-05T09:02:10.183502Z","iopub.status.idle":"2023-11-05T09:02:11.068069Z","shell.execute_reply.started":"2023-11-05T09:02:10.183444Z","shell.execute_reply":"2023-11-05T09:02:11.067009Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(input_sequences)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:02:14.293561Z","iopub.execute_input":"2023-11-05T09:02:14.294353Z","iopub.status.idle":"2023-11-05T09:02:14.300251Z","shell.execute_reply.started":"2023-11-05T09:02:14.294316Z","shell.execute_reply":"2023-11-05T09:02:14.299218Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[[   0    0    0 ...    0    5   13]\n [   0    0    0 ...    5   13    2]\n [   0    0    0 ...   13    2   43]\n ...\n [   0    0    0 ...   10 1785   11]\n [   0    0    0 ... 1785   11    1]\n [   0    0    0 ...   11    1   87]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Extract input sequences for model training\n# X contains all sequences except the last element\nX = input_sequences[:, :-1]\n\n# Extract the target values for each sequence\n# y contains the last element of each sequence\ny = input_sequences[:, -1]","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:02:17.455235Z","iopub.execute_input":"2023-11-05T09:02:17.456072Z","iopub.status.idle":"2023-11-05T09:02:17.460648Z","shell.execute_reply.started":"2023-11-05T09:02:17.456031Z","shell.execute_reply":"2023-11-05T09:02:17.459652Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Convert to One Hot Encoding Format\ny = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:02:19.969339Z","iopub.execute_input":"2023-11-05T09:02:19.970660Z","iopub.status.idle":"2023-11-05T09:02:26.287596Z","shell.execute_reply.started":"2023-11-05T09:02:19.970614Z","shell.execute_reply":"2023-11-05T09:02:26.286503Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Create a Sequential model\nmodel = Sequential()\n\n# Add an Embedding layer with vocabulary size, embedding dimension, and input length\nmodel.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n\n# Add an LSTM layer with 150 units\nmodel.add(LSTM(150))\n\n# Add a Dense layer with the number of total words in the vocabulary and softmax activation\nmodel.add(Dense(total_words, activation='softmax'))\n\n# Print the summary of the model architecture\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:02:40.232463Z","iopub.execute_input":"2023-11-05T09:02:40.233262Z","iopub.status.idle":"2023-11-05T09:02:45.295762Z","shell.execute_reply.started":"2023-11-05T09:02:40.233229Z","shell.execute_reply":"2023-11-05T09:02:45.293651Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 68, 100)           1167700   \n                                                                 \n lstm (LSTM)                 (None, 150)               150600    \n                                                                 \n dense (Dense)               (None, 11677)             1763227   \n                                                                 \n=================================================================\nTotal params: 3,081,527\nTrainable params: 3,081,527\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model on the input and output\nmodel.fit(X, y, epochs=100, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:30:27.011104Z","iopub.execute_input":"2023-11-05T09:30:27.011935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function to predict the next words\ndef predict(seed_text,next_words):\n    for _ in range(next_words):\n        token_list=tokenizer.texts_to_sequences([seed_text])[0]\n        \n        # Pad the sequence to match the input length of the model\n        token_list=pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        \n        # Predict the next word using the trained model\n        predicted = np.argmax(model.predict(token_list), axis=-1)\n        output_word=\"\"\n        \n        # Find the word corresponding to the predicted index in the vocabulary\n        for word, index in tokenizer.word_index.items():\n            if index==predicted:\n                output_word=word\n                break\n        seed_text+=\" \"+output_word\n        \n    print(seed_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get user input for the seed text\nseed_text=input(\"Enter your statement: \\n\")\n\n# Specify the number of words to predict\nnext_words=2\npredict(seed_text,next_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('predictormodel_kaggle.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}